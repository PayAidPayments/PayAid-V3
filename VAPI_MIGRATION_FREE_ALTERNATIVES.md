# VAPI Migration - Free/Open-Source Alternatives

**Date:** January 2026  
**Status:** Free Alternatives Analysis  
**Purpose:** Replace paid services with free/open-source solutions

---

## üìã Executive Summary

This document provides **free and open-source alternatives** to the paid services recommended in the VAPI migration guide. While paid services offer better performance and reliability, these free alternatives can work for development, testing, and low-volume production use cases.

**Trade-off:** Free alternatives typically require more setup, may have lower quality, and may not scale as well, but they eliminate ongoing costs.

---

## üîÑ Service-by-Service Free Alternatives

### 1. Telephony (Replace Twilio)

#### Option A: **FreePBX + Asterisk** ‚≠ê RECOMMENDED
- **Cost:** Free (self-hosted)
- **Type:** Open-source PBX system
- **Setup Complexity:** Medium-High
- **Features:**
  - Full PBX functionality
  - SIP trunking
  - Call routing
  - IVR support
  - Call recording
- **Requirements:**
  - VPS/server (DigitalOcean: $6/month minimum)
  - SIP provider (many free options)
  - Technical knowledge for setup
- **Pros:**
  - ‚úÖ Completely free (except hosting)
  - ‚úÖ Full control
  - ‚úÖ No per-minute charges
  - ‚úÖ Enterprise features
- **Cons:**
  - ‚ùå Requires server setup
  - ‚ùå More complex configuration
  - ‚ùå Need to handle infrastructure
- **Resources:**
  - https://www.freepbx.org/
  - https://www.asterisk.org/

#### Option B: **SIP.js + WebRTC** (Browser-based)
- **Cost:** Free
- **Type:** JavaScript library
- **Setup Complexity:** Medium
- **Features:**
  - WebRTC-based calling
  - Works in browser
  - SIP protocol support
- **Requirements:**
  - SIP server (can use free SIP providers)
  - WebRTC support in browser
- **Pros:**
  - ‚úÖ No server needed (client-side)
  - ‚úÖ Works in browser
  - ‚úÖ Free SIP providers available
- **Cons:**
  - ‚ùå Browser-only (not real phone numbers)
  - ‚ùå Limited to WebRTC-capable devices
  - ‚ùå Not suitable for traditional phone calls
- **Resources:**
  - https://sipjs.com/
  - Free SIP providers: Linphone, SIP2SIP

#### Option C: **Jitsi Meet** (WebRTC)
- **Cost:** Free (self-hosted)
- **Type:** Open-source video/voice conferencing
- **Setup Complexity:** Medium
- **Features:**
  - WebRTC-based
  - Can integrate with SIP
  - Self-hosted option
- **Pros:**
  - ‚úÖ Free and open-source
  - ‚úÖ Good documentation
  - ‚úÖ Active community
- **Cons:**
  - ‚ùå Primarily for conferencing
  - ‚ùå Not designed for voice agents
  - ‚ùå Requires customization
- **Resources:**
  - https://jitsi.org/

#### Option D: **Free SIP Providers** (Limited)
- **Cost:** Free (with limitations)
- **Examples:**
  - Linphone (free SIP account)
  - SIP2SIP (free SIP account)
  - VoIP.ms (very cheap, not free)
- **Limitations:**
  - Usually no real phone numbers (DID)
  - Limited minutes
  - May require paid plan for production

**Recommendation:** Use **FreePBX + Asterisk** if you have server access, or **SIP.js** for browser-based testing.

---

### 2. Speech-to-Text (Replace Deepgram)

#### Option A: **OpenAI Whisper** (Local) ‚≠ê RECOMMENDED
- **Cost:** Free (self-hosted)
- **Type:** Open-source model
- **Setup Complexity:** Medium
- **Features:**
  - High accuracy
  - Multi-language support
  - Can run locally
  - Streaming support (with modifications)
- **Requirements:**
  - GPU recommended (CPU works but slower)
  - Python environment
  - ~2-3GB model size
- **Performance:**
  - GPU: ~200-500ms latency
  - CPU: ~2-5 seconds latency
- **Pros:**
  - ‚úÖ Completely free
  - ‚úÖ High accuracy
  - ‚úÖ No API limits
  - ‚úÖ Privacy (runs locally)
- **Cons:**
  - ‚ùå Requires GPU for real-time
  - ‚ùå Higher latency than Deepgram
  - ‚ùå Need to set up streaming yourself
- **Resources:**
  - https://github.com/openai/whisper
  - https://github.com/ggerganov/whisper.cpp (C++ version, faster)

#### Option B: **Vosk** (Offline)
- **Cost:** Free
- **Type:** Offline speech recognition
- **Setup Complexity:** Low-Medium
- **Features:**
  - Runs completely offline
  - Multiple languages
  - Small model sizes
  - Real-time processing
- **Performance:**
  - ~100-300ms latency
- **Pros:**
  - ‚úÖ Very fast
  - ‚úÖ Works offline
  - ‚úÖ Small models (some < 50MB)
  - ‚úÖ Good for real-time
- **Cons:**
  - ‚ùå Lower accuracy than Whisper
  - ‚ùå Limited language support
  - ‚ùå May need multiple models for different languages
- **Resources:**
  - https://alphacephei.com/vosk/
  - https://github.com/alphacep/vosk-api

#### Option C: **Coqui STT** (Open-source)
- **Cost:** Free
- **Type:** Open-source STT
- **Setup Complexity:** Medium
- **Features:**
  - Based on DeepSpeech
  - Can train custom models
  - Streaming support
- **Pros:**
  - ‚úÖ Open-source
  - ‚úÖ Streaming support
  - ‚úÖ Customizable
- **Cons:**
  - ‚ùå Lower accuracy than Whisper
  - ‚ùå Requires more setup
- **Resources:**
  - https://github.com/coqui-ai/STT

#### Option D: **Mozilla DeepSpeech** (Legacy)
- **Cost:** Free
- **Type:** Open-source (legacy)
- **Note:** Project is archived, but still usable
- **Pros:**
  - ‚úÖ Free
  - ‚úÖ Offline
- **Cons:**
  - ‚ùå Project archived
  - ‚ùå Lower accuracy
  - ‚ùå Not actively maintained

**Recommendation:** Use **Whisper (local)** for best accuracy, or **Vosk** for lowest latency. You already have Whisper in your Docker setup!

---

### 3. Text-to-Speech (Replace ElevenLabs)

#### Option A: **Coqui TTS** ‚≠ê ALREADY IN YOUR SYSTEM!
- **Cost:** Free (you already have this!)
- **Type:** Open-source TTS
- **Status:** ‚úÖ Already implemented in your system
- **Features:**
  - High-quality voices
  - Multi-language support
  - Voice cloning
  - Streaming support
- **Performance:**
  - ~500-1000ms latency (slower than ElevenLabs)
  - Good quality
- **Pros:**
  - ‚úÖ Already set up in your Docker
  - ‚úÖ Completely free
  - ‚úÖ Good quality
  - ‚úÖ Multi-language
- **Cons:**
  - ‚ùå Slower than ElevenLabs
  - ‚ùå Requires GPU for best performance
- **Resources:**
  - Already in `docker-compose.ai-services.yml`
  - https://github.com/coqui-ai/TTS

#### Option B: **Piper TTS** (Lightweight)
- **Cost:** Free
- **Type:** Fast, lightweight TTS
- **Setup Complexity:** Low
- **Features:**
  - Very fast
  - Small models
  - Multiple voices
  - Works on CPU
- **Performance:**
  - ~100-300ms latency
- **Pros:**
  - ‚úÖ Very fast
  - ‚úÖ Low resource usage
  - ‚úÖ Works on CPU
  - ‚úÖ Good for real-time
- **Cons:**
  - ‚ùå Lower quality than Coqui/ElevenLabs
  - ‚ùå Less natural sounding
- **Resources:**
  - https://github.com/rhasspy/piper

#### Option C: **eSpeak-NG** (Basic)
- **Cost:** Free
- **Type:** Basic TTS engine
- **Features:**
  - Very fast
  - Multiple languages
  - Very small
- **Pros:**
  - ‚úÖ Extremely fast
  - ‚úÖ Very small footprint
  - ‚úÖ Many languages
- **Cons:**
  - ‚ùå Robotic/mechanical voice
  - ‚ùå Not suitable for production
- **Resources:**
  - https://github.com/espeak-ng/espeak-ng

#### Option D: **Google TTS (Free Tier)**
- **Cost:** Free (limited)
- **Type:** Cloud API
- **Limits:**
  - 4 million characters/month free
  - ~$4 per 1M characters after
- **Pros:**
  - ‚úÖ Good quality
  - ‚úÖ Easy to use
  - ‚úÖ Free tier available
- **Cons:**
  - ‚ùå Not completely free at scale
  - ‚ùå Requires internet
  - ‚ùå Privacy concerns

**Recommendation:** Use **Coqui TTS** (you already have it!) or **Piper TTS** for faster performance.

---

### 4. LLM (Replace OpenAI)

#### Option A: **Ollama** ‚≠ê ALREADY IN YOUR SYSTEM!
- **Cost:** Free (you already have this!)
- **Type:** Local LLM runner
- **Status:** ‚úÖ Already implemented
- **Features:**
  - Run models locally
  - Multiple model options
  - Streaming support
  - No API costs
- **Models Available:**
  - Llama 2/3 (free)
  - Mistral (free)
  - CodeLlama (free)
  - Many others
- **Performance:**
  - Depends on hardware
  - GPU: Fast
  - CPU: Slower but works
- **Pros:**
  - ‚úÖ Already set up
  - ‚úÖ Completely free
  - ‚úÖ Privacy (local)
  - ‚úÖ No API limits
  - ‚úÖ Streaming support
- **Cons:**
  - ‚ùå Requires good hardware
  - ‚ùå Slower than cloud APIs
  - ‚ùå May need GPU for real-time
- **Resources:**
  - Already in your system
  - https://ollama.ai/

#### Option B: **Groq (Free Tier)**
- **Cost:** Free (generous limits)
- **Type:** Cloud API (very fast)
- **Features:**
  - Extremely fast inference
  - Free tier available
  - Multiple models
- **Limits:**
  - ~30 requests/minute free
  - Good for testing
- **Pros:**
  - ‚úÖ Very fast
  - ‚úÖ Free tier
  - ‚úÖ Easy to use
- **Cons:**
  - ‚ùå Rate limits
  - ‚ùå Not completely free at scale
- **Resources:**
  - https://groq.com/

#### Option C: **Hugging Face Inference API (Free Tier)**
- **Cost:** Free (limited)
- **Type:** Cloud API
- **Features:**
  - Many open-source models
  - Free tier available
- **Limits:**
  - ~30,000 tokens/month free
- **Pros:**
  - ‚úÖ Many model options
  - ‚úÖ Free tier
  - ‚úÖ Easy to use
- **Cons:**
  - ‚ùå Rate limits
  - ‚ùå Slower than Groq

#### Option D: **Self-hosted Models (Llama, Mistral)**
- **Cost:** Free (self-hosted)
- **Type:** Run models on your server
- **Requirements:**
  - GPU recommended
  - Sufficient RAM
- **Pros:**
  - ‚úÖ Completely free
  - ‚úÖ Full control
  - ‚úÖ Privacy
- **Cons:**
  - ‚ùå Requires powerful hardware
  - ‚ùå Setup complexity

**Recommendation:** Use **Ollama** (you already have it!) - it's perfect for this use case.

---

## üéØ Recommended Free Stack

### Option 1: **Fully Free (Self-Hosted)** ‚≠ê BEST FOR FREE

```
Telephony:     FreePBX + Asterisk (self-hosted)
STT:           OpenAI Whisper (local) or Vosk
TTS:           Coqui TTS (already in your system!)
LLM:           Ollama (already in your system!)
```

**Cost:** Only server hosting (~$6-20/month for VPS)

**Pros:**
- ‚úÖ Completely free (except hosting)
- ‚úÖ Full control
- ‚úÖ Privacy
- ‚úÖ No API limits

**Cons:**
- ‚ùå Requires server setup
- ‚ùå More complex
- ‚ùå Need to maintain infrastructure

---

### Option 2: **Hybrid (Free + Free Tiers)**

```
Telephony:     FreePBX + Asterisk
STT:           OpenAI Whisper (local)
TTS:           Coqui TTS (already have it!)
LLM:           Ollama (already have it!)
```

**Cost:** Only server hosting

**Same as Option 1, but uses what you already have!**

---

### Option 3: **Browser-Based (No Telephony)**

```
Telephony:     SIP.js + WebRTC (browser)
STT:           OpenAI Whisper (local API)
TTS:           Coqui TTS (local API)
LLM:           Ollama (local API)
```

**Cost:** Free (but limited to browser)

**Note:** This is similar to your current system, but with better STT/TTS/LLM.

---

## üìä Comparison: Paid vs Free

| Service | Paid (VAPI-style) | Free Alternative | Trade-off |
|---------|------------------|------------------|-----------|
| **Telephony** | Twilio ($0.009/min) | FreePBX (free) | Setup complexity |
| **STT** | Deepgram ($0.0043/min) | Whisper local (free) | Latency (2-5s vs 200ms) |
| **TTS** | ElevenLabs ($0.18/1K chars) | Coqui TTS (free) | Latency (500ms vs 200ms) |
| **LLM** | OpenAI ($0.01/1K tokens) | Ollama (free) | Speed (local vs cloud) |
| **Total Cost** | ~$65-90/month | ~$6-20/month (hosting) | **Savings: ~$45-70/month** |

---

## ‚ö†Ô∏è Important Considerations

### Latency Impact

**Paid Services:**
- Total latency: 400-600ms
- Optimized for real-time

**Free Alternatives:**
- Total latency: 1-3 seconds (with GPU)
- May be 3-5 seconds (with CPU only)
- **Still better than your current 2-5 seconds!**

### Quality Impact

**Paid Services:**
- Professional-grade quality
- Optimized models
- High accuracy

**Free Alternatives:**
- Good quality (especially Whisper)
- May need tuning
- Slightly lower accuracy in some cases

### Scalability

**Paid Services:**
- Auto-scales
- Handles high load
- 99.99% uptime

**Free Alternatives:**
- Limited by your hardware
- Need to scale manually
- Uptime depends on your infrastructure

---

## üöÄ Implementation Strategy for Free Stack

### Phase 1: Use What You Have ‚úÖ

You already have:
- ‚úÖ **Coqui TTS** (in Docker)
- ‚úÖ **Ollama** (can be added to Docker)
- ‚úÖ **Whisper** (can use your existing STT service)

**Action:** Modify your existing orchestrator to use these!

### Phase 2: Add Free Telephony

1. **Option A:** Set up FreePBX on a VPS
2. **Option B:** Use SIP.js for browser-based testing first
3. **Option C:** Use a free SIP provider (limited)

### Phase 3: Optimize for Performance

1. Use GPU for Whisper (if available)
2. Optimize Coqui TTS settings
3. Use faster Ollama models (Mistral, Llama 3)
4. Implement caching

---

## üí° Recommended Approach

### For Development/Testing:
```
‚úÖ Use your existing Coqui TTS
‚úÖ Use Ollama (add to Docker)
‚úÖ Use Whisper (already in your STT service)
‚úÖ Test with SIP.js in browser first
```

### For Production (Low Volume):
```
‚úÖ FreePBX + Asterisk (self-hosted)
‚úÖ Whisper (local, GPU if possible)
‚úÖ Coqui TTS (already have it)
‚úÖ Ollama (already can use it)
```

### For Production (High Volume):
```
‚ö†Ô∏è Consider paid services for:
   - Better latency
   - Auto-scaling
   - Reliability
   - Support
```

---

## üìù Next Steps (Free Implementation)

1. **Keep Your Current Setup:**
   - Coqui TTS (already working)
   - Add Ollama to Docker
   - Use Whisper from your STT service

2. **Add Free Telephony:**
   - Start with SIP.js for testing
   - Move to FreePBX when ready

3. **Optimize:**
   - Use GPU if available
   - Cache responses
   - Optimize model sizes

---

## üéØ Cost Comparison

### Paid Stack (VAPI-style):
- **Monthly:** $65-90
- **Per 1,000 minutes:** ~$65-90

### Free Stack:
- **Monthly:** $6-20 (VPS hosting only)
- **Per 1,000 minutes:** ~$6-20
- **Savings:** ~$45-70/month (50-80% savings!)

---

## ‚ö†Ô∏è Trade-offs Summary

| Aspect | Paid Services | Free Alternatives |
|--------|--------------|-------------------|
| **Cost** | $65-90/month | $6-20/month |
| **Latency** | 400-600ms | 1-3 seconds |
| **Setup** | Easy | Complex |
| **Maintenance** | Low | High |
| **Scalability** | Auto | Manual |
| **Quality** | Excellent | Good |
| **Reliability** | 99.99% | Depends on you |

---

## üéì Recommendation

**For your use case (PayAid V3):**

1. **Start with Free Stack:**
   - Use Coqui TTS (already have it)
   - Use Ollama (add to Docker)
   - Use Whisper (from your STT service)
   - Test with SIP.js first

2. **If Performance is Acceptable:**
   - Continue with free stack
   - Optimize as needed

3. **If You Need Better Performance:**
   - Consider paid services for critical components
   - Or hybrid approach (free TTS/LLM, paid STT)

4. **For Production:**
   - Free stack works for low-medium volume
   - Paid stack better for high volume/enterprise

---

**Document Version:** 1.0  
**Last Updated:** January 2026  
**Status:** Recommendations Only (No Implementation)

---

## üîó Resources

- **FreePBX:** https://www.freepbx.org/
- **Asterisk:** https://www.asterisk.org/
- **Whisper:** https://github.com/openai/whisper
- **Vosk:** https://alphacephei.com/vosk/
- **Coqui TTS:** https://github.com/coqui-ai/TTS (already in your system!)
- **Ollama:** https://ollama.ai/
- **Piper TTS:** https://github.com/rhasspy/piper
- **SIP.js:** https://sipjs.com/

---

**Note:** This document provides suggestions only. Implementation should be done carefully with proper testing.
